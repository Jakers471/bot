{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training for Market Regime Classification\n",
    "\n",
    "This notebook trains machine learning models to classify market regimes based on labeled technical indicators.\n",
    "\n",
    "**Label Classes:**\n",
    "- 0 = ranging (sideways movement)\n",
    "- 1 = trending_up (bullish trend)\n",
    "- 2 = trending_down (bearish trend)\n",
    "\n",
    "**Workflow:**\n",
    "1. Load features and labels\n",
    "2. Prepare training data\n",
    "3. Train XGBoost classifier\n",
    "4. Evaluate performance\n",
    "5. Compare with RandomForest\n",
    "6. Save best model for backtesting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Import required libraries and modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "Features directory: C:\\Users\\jakers\\Desktop\\bot\\features\n",
      "Labels directory: C:\\Users\\jakers\\Desktop\\bot\\labels\n",
      "Models directory: C:\\Users\\jakers\\Desktop\\bot\\models\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "# Import trainer functions\n",
    "from src.trainer import (\n",
    "    train_model,\n",
    "    evaluate_model,\n",
    "    save_model,\n",
    "    get_feature_importance,\n",
    "    prepare_features,\n",
    "    cross_validate\n",
    ")\n",
    "from src.config import FEATURES_DIR, LABELS_DIR, MODELS_DIR, RANDOM_STATE, TRAIN_TEST_SPLIT\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Features directory: {FEATURES_DIR}\")\n",
    "print(f\"Labels directory: {LABELS_DIR}\")\n",
    "print(f\"Models directory: {MODELS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Features & Labels\n",
    "\n",
    "Load the features from parquet files and labels from JSON, then merge them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading features from: C:\\Users\\jakers\\Desktop\\bot\\features\\BTCUSDT_1h_features.parquet\n",
      "\n",
      "Features loaded: 51864 rows, 232 columns\n",
      "Date range: 2020-01-01 00:00:00 to 2025-11-30 23:00:00\n",
      "\n",
      "Loading labels from: C:\\Users\\jakers\\Desktop\\bot\\labels\\BTCUSDT_1h_labels.json\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Convert labels dict to DataFrame\u001b[39;00m\n\u001b[32m     39\u001b[39m labels_list = []\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m timestamp, label_info \u001b[38;5;129;01min\u001b[39;00m \u001b[43mlabels_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlabels\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m():\n\u001b[32m     41\u001b[39m     labels_list.append({\n\u001b[32m     42\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mopen_time\u001b[39m\u001b[33m'\u001b[39m: pd.to_datetime(\u001b[38;5;28mint\u001b[39m(timestamp), unit=\u001b[33m'\u001b[39m\u001b[33mms\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m     43\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m: label_info[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     44\u001b[39m     })\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m labels_list:\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'items'"
     ]
    }
   ],
   "source": "# Configuration\nSYMBOL = 'BTCUSDT'\nINTERVAL = '1h'\n\n# Load features from parquet\nfeatures_path = FEATURES_DIR / f\"{SYMBOL}_{INTERVAL}_features.parquet\"\nprint(f\"Loading features from: {features_path}\")\n\nif not features_path.exists():\n    raise FileNotFoundError(f\"Features file not found: {features_path}\")\n\ndf_features = pd.read_parquet(features_path)\nprint(f\"\\nFeatures loaded: {len(df_features)} rows, {len(df_features.columns)} columns\")\nprint(f\"Date range: {df_features['open_time'].min()} to {df_features['open_time'].max()}\")\n\n# Load labels from JSON\nlabels_path = LABELS_DIR / f\"{SYMBOL}_{INTERVAL}_labels.json\"\nprint(f\"\\nLoading labels from: {labels_path}\")\n\nif not labels_path.exists():\n    print(f\"\\nWARNING: Labels file not found: {labels_path}\")\n    print(\"Please run the labeling notebook (02_label_data.ipynb) first to create labels.\")\n    print(\"\\nFor demonstration purposes, we'll create sample labels...\")\n    \n    # Create sample labels for demonstration\n    labels_data = {\n        'metadata': {\n            'symbol': SYMBOL,\n            'interval': INTERVAL,\n            'total_labels': 0\n        },\n        'labels': {}\n    }\nelse:\n    with open(labels_path, 'r') as f:\n        labels_data = json.load(f)\n\n# Convert labels dict to DataFrame\nlabels_list = []\nfor label_info in labels_data.get('labels', []):\n    labels_list.append({\n        # Label spans start_idx to end_idx\n        'label': label_info['label']\n    })\n\nif labels_list:\n    df_labels = pd.DataFrame(labels_list)\n    print(f\"\\nLabels loaded: {len(df_labels)} labeled samples\")\n    print(f\"\\nLabel distribution:\")\n    label_counts = df_labels['label'].value_counts().sort_index()\n    for label, count in label_counts.items():\n        label_name = ['ranging', 'trending_up', 'trending_down'][label]\n        print(f\"  {label} ({label_name}): {count} ({count/len(df_labels)*100:.1f}%)\")\nelse:\n    print(\"\\nNo labels found. Please label your data first!\")\n    df_labels = pd.DataFrame(columns=['open_time', 'label'])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge features and labels\n",
    "df = df_features.merge(df_labels, on='open_time', how='left')\n",
    "print(f\"Merged dataset: {len(df)} rows\")\n",
    "print(f\"Labeled samples: {df['label'].notna().sum()}\")\n",
    "print(f\"Unlabeled samples: {df['label'].isna().sum()}\")\n",
    "\n",
    "# Show sample of merged data\n",
    "print(\"\\nSample of merged data:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Training Data\n",
    "\n",
    "Filter to only labeled rows and separate features (X) from labels (y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to labeled data only\n",
    "df_labeled = df.dropna(subset=['label']).copy()\n",
    "print(f\"Labeled dataset: {len(df_labeled)} samples\")\n",
    "\n",
    "if len(df_labeled) == 0:\n",
    "    raise ValueError(\"No labeled data available. Please label data first using 02_label_data.ipynb\")\n",
    "\n",
    "# Check for minimum samples per class\n",
    "min_samples_per_class = df_labeled['label'].value_counts().min()\n",
    "print(f\"\\nMinimum samples per class: {min_samples_per_class}\")\n",
    "\n",
    "if min_samples_per_class < 10:\n",
    "    print(\"\\nWARNING: Very few samples in some classes. Consider labeling more data for better model performance.\")\n",
    "\n",
    "# Prepare features and target\n",
    "y = df_labeled['label'].values.astype(int)\n",
    "X, feature_names = prepare_features(df_labeled)\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "print(f\"Number of features: {len(feature_names)}\")\n",
    "print(f\"Target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Selection\n",
    "\n",
    "Display which features will be used for training (excludes raw OHLCV and timestamps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total features for training: {len(feature_names)}\\n\")\n",
    "print(\"Features being used:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Group features by category for better readability\n",
    "ma_features = [f for f in feature_names if 'ma_' in f or 'ema_' in f]\n",
    "spread_features = [f for f in feature_names if 'spread' in f]\n",
    "slope_features = [f for f in feature_names if 'slope' in f]\n",
    "compression_features = [f for f in feature_names if 'compression' in f]\n",
    "position_features = [f for f in feature_names if 'position' in f]\n",
    "other_features = [f for f in feature_names if f not in ma_features + spread_features + \n",
    "                  slope_features + compression_features + position_features]\n",
    "\n",
    "if ma_features:\n",
    "    print(f\"\\nMoving Averages ({len(ma_features)}):\")\n",
    "    for f in ma_features[:10]:  # Show first 10\n",
    "        print(f\"  - {f}\")\n",
    "    if len(ma_features) > 10:\n",
    "        print(f\"  ... and {len(ma_features)-10} more\")\n",
    "\n",
    "if spread_features:\n",
    "    print(f\"\\nSpread Features ({len(spread_features)}):\")\n",
    "    for f in spread_features:\n",
    "        print(f\"  - {f}\")\n",
    "\n",
    "if slope_features:\n",
    "    print(f\"\\nSlope Features ({len(slope_features)}):\")\n",
    "    for f in slope_features:\n",
    "        print(f\"  - {f}\")\n",
    "\n",
    "if compression_features:\n",
    "    print(f\"\\nCompression Features ({len(compression_features)}):\")\n",
    "    for f in compression_features:\n",
    "        print(f\"  - {f}\")\n",
    "\n",
    "if position_features:\n",
    "    print(f\"\\nPosition Features ({len(position_features)}):\")\n",
    "    for f in position_features:\n",
    "        print(f\"  - {f}\")\n",
    "\n",
    "if other_features:\n",
    "    print(f\"\\nOther Features ({len(other_features)}):\")\n",
    "    for f in other_features:\n",
    "        print(f\"  - {f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"\\nExcluded columns (metadata, not features):\")\n",
    "excluded = ['open_time', 'label', 'timestamp', 'date', 'datetime', \n",
    "            'close_time', 'open', 'high', 'low', 'close', 'volume']\n",
    "actual_excluded = [col for col in excluded if col in df_labeled.columns]\n",
    "for col in actual_excluded:\n",
    "    print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train/Test Split\n",
    "\n",
    "Split data into training (80%) and testing (20%) sets with stratification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform stratified train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=(1 - TRAIN_TEST_SPLIT),\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train/Test Split Configuration:\")\n",
    "print(f\"  Split ratio: {TRAIN_TEST_SPLIT:.0%} train / {(1-TRAIN_TEST_SPLIT):.0%} test\")\n",
    "print(f\"  Random state: {RANDOM_STATE}\")\n",
    "print(f\"  Stratified: Yes\\n\")\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set: {len(X_test)} samples ({len(X_test)/len(X)*100:.1f}%)\\n\")\n",
    "\n",
    "# Show class distribution in train and test sets\n",
    "train_dist = pd.Series(y_train).value_counts().sort_index()\n",
    "test_dist = pd.Series(y_test).value_counts().sort_index()\n",
    "\n",
    "print(\"Class distribution in training set:\")\n",
    "for label, count in train_dist.items():\n",
    "    label_name = ['ranging', 'trending_up', 'trending_down'][label]\n",
    "    print(f\"  {label} ({label_name}): {count} ({count/len(y_train)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nClass distribution in test set:\")\n",
    "for label, count in test_dist.items():\n",
    "    label_name = ['ranging', 'trending_up', 'trending_down'][label]\n",
    "    print(f\"  {label} ({label_name}): {count} ({count/len(y_test)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train XGBoost Model\n",
    "\n",
    "Train an XGBoost classifier with optimized hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training XGBoost classifier...\\n\")\n",
    "print(\"Model Configuration:\")\n",
    "print(\"  Algorithm: XGBoost\")\n",
    "print(\"  n_estimators: 100\")\n",
    "print(\"  max_depth: 6\")\n",
    "print(\"  learning_rate: 0.1\")\n",
    "print(\"  subsample: 0.8\")\n",
    "print(\"  colsample_bytree: 0.8\")\n",
    "print(\"  eval_metric: mlogloss\")\n",
    "print()\n",
    "\n",
    "# Train XGBoost model\n",
    "xgb_model = train_model(X_train, y_train, model_type='xgboost')\n",
    "\n",
    "print(\"XGBoost model trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate XGBoost Performance\n",
    "\n",
    "Calculate accuracy, precision, recall, F1 score, and visualize the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate XGBoost model\n",
    "xgb_metrics = evaluate_model(xgb_model, X_test, y_test)\n",
    "\n",
    "print(\"XGBoost Test Set Performance:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Accuracy:  {xgb_metrics['accuracy']:.4f}\")\n",
    "print(f\"Precision: {xgb_metrics['precision']:.4f}\")\n",
    "print(f\"Recall:    {xgb_metrics['recall']:.4f}\")\n",
    "print(f\"F1 Score:  {xgb_metrics['f1']:.4f}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix as heatmap\n",
    "cm = np.array(xgb_metrics['confusion_matrix'])\n",
    "labels = ['Ranging', 'Trending Up', 'Trending Down']\n",
    "\n",
    "# Create heatmap\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=cm,\n",
    "    x=labels,\n",
    "    y=labels,\n",
    "    text=cm,\n",
    "    texttemplate='%{text}',\n",
    "    textfont={\"size\": 16},\n",
    "    colorscale='Blues',\n",
    "    showscale=True\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='XGBoost Confusion Matrix',\n",
    "    xaxis_title='Predicted Label',\n",
    "    yaxis_title='True Label',\n",
    "    width=600,\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Calculate per-class metrics\n",
    "print(\"\\nPer-Class Performance:\")\n",
    "print(\"=\" * 50)\n",
    "for i, label_name in enumerate(labels):\n",
    "    tp = cm[i, i]\n",
    "    fp = cm[:, i].sum() - tp\n",
    "    fn = cm[i, :].sum() - tp\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    print(f\"{label_name}:\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall:    {recall:.4f}\")\n",
    "    print(f\"  F1 Score:  {f1:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Importance\n",
    "\n",
    "Visualize the top 20 most important features for the XGBoost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "importance_df = get_feature_importance(xgb_model, feature_names)\n",
    "\n",
    "print(\"Top 20 Most Important Features:\")\n",
    "print(\"=\" * 50)\n",
    "print(importance_df.head(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top 20 feature importances\n",
    "top_n = 20\n",
    "top_features = importance_df.head(top_n)\n",
    "\n",
    "fig = go.Figure(go.Bar(\n",
    "    x=top_features['importance'],\n",
    "    y=top_features['feature'],\n",
    "    orientation='h',\n",
    "    marker=dict(\n",
    "        color=top_features['importance'],\n",
    "        colorscale='Viridis',\n",
    "        showscale=True\n",
    "    )\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f'Top {top_n} Most Important Features (XGBoost)',\n",
    "    xaxis_title='Importance Score',\n",
    "    yaxis_title='Feature',\n",
    "    height=600,\n",
    "    width=900,\n",
    "    yaxis={'autorange': 'reversed'}  # Most important at top\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Cross-Validation\n",
    "\n",
    "Perform 5-fold cross-validation to assess model stability and generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Performing 5-fold cross-validation on XGBoost...\\n\")\n",
    "\n",
    "# Perform cross-validation on full labeled dataset\n",
    "cv_results = cross_validate(X, y, model_type='xgboost', cv=5)\n",
    "\n",
    "print(\"Cross-Validation Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Fold scores: {[f'{s:.4f}' for s in cv_results['scores']]}\")\n",
    "print(f\"\\nMean CV Score: {cv_results['mean_score']:.4f}\")\n",
    "print(f\"Std CV Score:  {cv_results['std_score']:.4f}\")\n",
    "print(f\"95% CI:        {cv_results['mean_score']:.4f} +/- {1.96 * cv_results['std_score']:.4f}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CV scores\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add bars for each fold\n",
    "fig.add_trace(go.Bar(\n",
    "    x=[f'Fold {i+1}' for i in range(len(cv_results['scores']))],\n",
    "    y=cv_results['scores'],\n",
    "    name='CV Score',\n",
    "    marker_color='lightblue'\n",
    "))\n",
    "\n",
    "# Add mean line\n",
    "fig.add_hline(\n",
    "    y=cv_results['mean_score'],\n",
    "    line_dash=\"dash\",\n",
    "    line_color=\"red\",\n",
    "    annotation_text=f\"Mean: {cv_results['mean_score']:.4f}\",\n",
    "    annotation_position=\"right\"\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='5-Fold Cross-Validation Scores (XGBoost)',\n",
    "    xaxis_title='Fold',\n",
    "    yaxis_title='Accuracy Score',\n",
    "    width=800,\n",
    "    height=500,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Compare Models\n",
    "\n",
    "Train a RandomForest model and compare performance with XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training RandomForest classifier...\\n\")\n",
    "print(\"Model Configuration:\")\n",
    "print(\"  Algorithm: RandomForest\")\n",
    "print(\"  n_estimators: 100\")\n",
    "print(\"  max_depth: 10\")\n",
    "print(\"  min_samples_split: 5\")\n",
    "print(\"  min_samples_leaf: 2\")\n",
    "print()\n",
    "\n",
    "# Train RandomForest model\n",
    "rf_model = train_model(X_train, y_train, model_type='randomforest')\n",
    "\n",
    "print(\"RandomForest model trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate RandomForest model\n",
    "rf_metrics = evaluate_model(rf_model, X_test, y_test)\n",
    "\n",
    "print(\"RandomForest Test Set Performance:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Accuracy:  {rf_metrics['accuracy']:.4f}\")\n",
    "print(f\"Precision: {rf_metrics['precision']:.4f}\")\n",
    "print(f\"Recall:    {rf_metrics['recall']:.4f}\")\n",
    "print(f\"F1 Score:  {rf_metrics['f1']:.4f}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare both models side by side\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "    'XGBoost': [\n",
    "        xgb_metrics['accuracy'],\n",
    "        xgb_metrics['precision'],\n",
    "        xgb_metrics['recall'],\n",
    "        xgb_metrics['f1']\n",
    "    ],\n",
    "    'RandomForest': [\n",
    "        rf_metrics['accuracy'],\n",
    "        rf_metrics['precision'],\n",
    "        rf_metrics['recall'],\n",
    "        rf_metrics['f1']\n",
    "    ]\n",
    "})\n",
    "\n",
    "comparison_df['Difference'] = comparison_df['XGBoost'] - comparison_df['RandomForest']\n",
    "\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(\"=\" * 70)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Determine best model\n",
    "if xgb_metrics['f1'] > rf_metrics['f1']:\n",
    "    best_model = xgb_model\n",
    "    best_model_name = 'XGBoost'\n",
    "    best_metrics = xgb_metrics\n",
    "else:\n",
    "    best_model = rf_model\n",
    "    best_model_name = 'RandomForest'\n",
    "    best_metrics = rf_metrics\n",
    "\n",
    "print(f\"\\nBest Model: {best_model_name} (F1 Score: {best_metrics['f1']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig = go.Figure()\n",
    "\n",
    "metrics_list = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "xgb_scores = comparison_df['XGBoost'].values\n",
    "rf_scores = comparison_df['RandomForest'].values\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    name='XGBoost',\n",
    "    x=metrics_list,\n",
    "    y=xgb_scores,\n",
    "    marker_color='blue'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    name='RandomForest',\n",
    "    x=metrics_list,\n",
    "    y=rf_scores,\n",
    "    marker_color='green'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Model Performance Comparison',\n",
    "    xaxis_title='Metric',\n",
    "    yaxis_title='Score',\n",
    "    barmode='group',\n",
    "    width=800,\n",
    "    height=500,\n",
    "    yaxis=dict(range=[0, 1])\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Best Model\n",
    "\n",
    "Save the best performing model to the models directory for use in backtesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "model_path = save_model(best_model, SYMBOL, INTERVAL)\n",
    "\n",
    "print(f\"Best model ({best_model_name}) saved successfully!\")\n",
    "print(f\"\\nModel file: {model_path}\")\n",
    "print(f\"Model size: {model_path.stat().st_size / 1024:.2f} KB\")\n",
    "\n",
    "# Save model metadata\n",
    "metadata = {\n",
    "    'symbol': SYMBOL,\n",
    "    'interval': INTERVAL,\n",
    "    'model_type': best_model_name,\n",
    "    'training_date': pd.Timestamp.now().isoformat(),\n",
    "    'n_samples_train': len(X_train),\n",
    "    'n_samples_test': len(X_test),\n",
    "    'n_features': len(feature_names),\n",
    "    'metrics': {\n",
    "        'accuracy': float(best_metrics['accuracy']),\n",
    "        'precision': float(best_metrics['precision']),\n",
    "        'recall': float(best_metrics['recall']),\n",
    "        'f1': float(best_metrics['f1'])\n",
    "    },\n",
    "    'feature_names': feature_names,\n",
    "    'label_mapping': {\n",
    "        '0': 'ranging',\n",
    "        '1': 'trending_up',\n",
    "        '2': 'trending_down'\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_path = MODELS_DIR / f\"{SYMBOL}_{INTERVAL}_model_metadata.json\"\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"\\nMetadata saved: {metadata_path}\")\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL TRAINING COMPLETE!\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nThe trained {best_model_name} model is ready for backtesting.\")\n",
    "print(f\"Use this model file in your backtesting notebook: {model_path.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook successfully:\n",
    "1. Loaded features from parquet files and labels from JSON\n",
    "2. Prepared and split the training data (80/20 stratified split)\n",
    "3. Trained an XGBoost classifier\n",
    "4. Evaluated model performance with detailed metrics\n",
    "5. Visualized feature importance\n",
    "6. Performed 5-fold cross-validation\n",
    "7. Compared XGBoost with RandomForest\n",
    "8. Saved the best model for backtesting\n",
    "\n",
    "**Next Steps:**\n",
    "- Use the saved model in backtesting (04_backtest.ipynb)\n",
    "- If model performance is poor, consider:\n",
    "  - Labeling more data\n",
    "  - Engineering additional features\n",
    "  - Tuning hyperparameters\n",
    "  - Trying different model architectures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
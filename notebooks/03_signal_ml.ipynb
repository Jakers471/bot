{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Signal Discovery\n",
    "\n",
    "This notebook uses machine learning to discover trading signals within confirmed market regimes.\n",
    "\n",
    "**Philosophy:**\n",
    "- Regime detection is rule-based (from notebook 02)\n",
    "- ML finds *when* to trade within each regime\n",
    "- The model learns patterns that precede profitable moves\n",
    "\n",
    "**Approach:**\n",
    "1. Load data with auto-generated regime labels\n",
    "2. Create target labels based on future price movement\n",
    "3. Train ML to predict good entry points\n",
    "4. Evaluate signal quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "\n",
    "# Add parent to path\n",
    "sys.path.insert(0, str(Path('..').resolve()))\n",
    "from src.config import FEATURES_DIR, MODELS_DIR, LABELS_DIR\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Labeled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYMBOL = 'BTCUSDT'\n",
    "INTERVAL = '1h'\n",
    "\n",
    "# Try to load labeled features first, fall back to regular features\n",
    "labeled_path = FEATURES_DIR / f'{SYMBOL}_{INTERVAL}_features_labeled.parquet'\n",
    "regular_path = FEATURES_DIR / f'{SYMBOL}_{INTERVAL}_features.parquet'\n",
    "\n",
    "if labeled_path.exists():\n",
    "    print(f\"Loading labeled features from: {labeled_path}\")\n",
    "    df = pd.read_parquet(labeled_path)\n",
    "    has_regime = 'regime' in df.columns\n",
    "else:\n",
    "    print(f\"Labeled features not found. Loading regular features from: {regular_path}\")\n",
    "    print(\"Note: Run notebook 02 to generate regime labels first!\")\n",
    "    df = pd.read_parquet(regular_path)\n",
    "    has_regime = False\n",
    "\n",
    "df['open_time'] = pd.to_datetime(df['open_time'])\n",
    "df = df.sort_values('open_time').reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nLoaded {len(df):,} rows\")\n",
    "print(f\"Date range: {df['open_time'].min()} to {df['open_time'].max()}\")\n",
    "print(f\"Has regime labels: {has_regime}\")\n",
    "\n",
    "if has_regime:\n",
    "    print(f\"\\nRegime distribution:\")\n",
    "    regime_map = {0: 'Ranging', 1: 'Trending Up', 2: 'Trending Down'}\n",
    "    for r, name in regime_map.items():\n",
    "        count = (df['regime'] == r).sum()\n",
    "        print(f\"  {name}: {count:,} ({count/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Target Labels\n",
    "\n",
    "We create forward-looking labels based on actual price movement:\n",
    "- **Good Long Entry**: Price increases by X% within N bars\n",
    "- **Good Short Entry**: Price decreases by X% within N bars\n",
    "- **No Trade**: Neither condition met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target parameters\n",
    "LOOKAHEAD_BARS = 24  # How far ahead to look (24 hours for 1h data)\n",
    "PROFIT_TARGET_PCT = 1.5  # Minimum % gain to be considered \"good entry\"\n",
    "STOP_LOSS_PCT = 1.0  # Maximum % loss before exit (for risk-adjusted signals)\n",
    "\n",
    "print(f\"Target Configuration:\")\n",
    "print(f\"  Lookahead: {LOOKAHEAD_BARS} bars\")\n",
    "print(f\"  Profit Target: {PROFIT_TARGET_PCT}%\")\n",
    "print(f\"  Stop Loss: {STOP_LOSS_PCT}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_signal_targets(df, lookahead=24, profit_pct=1.5, stop_pct=1.0):\n",
    "    \"\"\"\n",
    "    Create target labels based on future price movement.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with added columns:\n",
    "        - future_return: Max favorable return in lookahead period\n",
    "        - future_drawdown: Max adverse return in lookahead period\n",
    "        - signal_long: 1 if good long entry, 0 otherwise\n",
    "        - signal_short: 1 if good short entry, 0 otherwise\n",
    "        - signal: Combined (0=none, 1=long, 2=short)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    n = len(df)\n",
    "    \n",
    "    # Calculate future highs/lows\n",
    "    future_high = np.full(n, np.nan)\n",
    "    future_low = np.full(n, np.nan)\n",
    "    \n",
    "    for i in range(n - lookahead):\n",
    "        future_slice = df.iloc[i+1:i+lookahead+1]\n",
    "        future_high[i] = future_slice['high'].max()\n",
    "        future_low[i] = future_slice['low'].min()\n",
    "    \n",
    "    df['future_high'] = future_high\n",
    "    df['future_low'] = future_low\n",
    "    \n",
    "    # Calculate potential returns\n",
    "    df['future_return_up'] = (df['future_high'] - df['close']) / df['close'] * 100\n",
    "    df['future_return_down'] = (df['close'] - df['future_low']) / df['close'] * 100\n",
    "    df['future_drawdown_long'] = (df['close'] - df['future_low']) / df['close'] * 100\n",
    "    df['future_drawdown_short'] = (df['future_high'] - df['close']) / df['close'] * 100\n",
    "    \n",
    "    # Define good entries\n",
    "    # Long: Price goes up by profit_pct without first dropping by stop_pct\n",
    "    df['signal_long'] = (\n",
    "        (df['future_return_up'] >= profit_pct) & \n",
    "        (df['future_drawdown_long'] < stop_pct)\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Short: Price goes down by profit_pct without first rising by stop_pct\n",
    "    df['signal_short'] = (\n",
    "        (df['future_return_down'] >= profit_pct) & \n",
    "        (df['future_drawdown_short'] < stop_pct)\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Combined signal (prioritize based on regime if available)\n",
    "    df['signal'] = 0  # No trade\n",
    "    df.loc[df['signal_long'] == 1, 'signal'] = 1  # Long\n",
    "    df.loc[df['signal_short'] == 1, 'signal'] = 2  # Short\n",
    "    # If both, prefer based on regime\n",
    "    if 'regime' in df.columns:\n",
    "        both = (df['signal_long'] == 1) & (df['signal_short'] == 1)\n",
    "        df.loc[both & (df['regime'] == 1), 'signal'] = 1  # Uptrend -> prefer long\n",
    "        df.loc[both & (df['regime'] == 2), 'signal'] = 2  # Downtrend -> prefer short\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create targets\n",
    "df = create_signal_targets(df, LOOKAHEAD_BARS, PROFIT_TARGET_PCT, STOP_LOSS_PCT)\n",
    "\n",
    "print(f\"\\nSignal Distribution:\")\n",
    "signal_map = {0: 'No Trade', 1: 'Long', 2: 'Short'}\n",
    "for s, name in signal_map.items():\n",
    "    count = (df['signal'] == s).sum()\n",
    "    print(f\"  {name}: {count:,} ({count/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Check by regime if available\n",
    "if has_regime:\n",
    "    print(\"\\nSignals by Regime:\")\n",
    "    for r, rname in {0: 'Ranging', 1: 'Trending Up', 2: 'Trending Down'}.items():\n",
    "        regime_df = df[df['regime'] == r]\n",
    "        if len(regime_df) > 0:\n",
    "            long_pct = (regime_df['signal'] == 1).mean() * 100\n",
    "            short_pct = (regime_df['signal'] == 2).mean() * 100\n",
    "            print(f\"  {rname}: Long={long_pct:.1f}%, Short={short_pct:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to exclude from features\n",
    "exclude_cols = [\n",
    "    # Metadata\n",
    "    'open_time', 'close_time', 'timestamp',\n",
    "    # Raw OHLCV (we keep derived features)\n",
    "    'open', 'high', 'low', 'close', 'volume',\n",
    "    'quote_volume', 'trades', 'taker_buy_base', 'taker_buy_quote', 'ignore',\n",
    "    # Target columns (would be cheating!)\n",
    "    'future_high', 'future_low', 'future_return_up', 'future_return_down',\n",
    "    'future_drawdown_long', 'future_drawdown_short',\n",
    "    'signal_long', 'signal_short', 'signal',\n",
    "    # Regime (use as filter, not feature)\n",
    "    'regime', 'regime_name', 'raw_regime'\n",
    "]\n",
    "\n",
    "# Get feature columns\n",
    "feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "print(f\"Total features: {len(feature_cols)}\")\n",
    "\n",
    "# Group features by type\n",
    "ma_features = [f for f in feature_cols if f.startswith('ma') and '_' in f]\n",
    "spread_features = [f for f in feature_cols if 'spread' in f]\n",
    "slope_features = [f for f in feature_cols if 'slope' in f]\n",
    "other_features = [f for f in feature_cols if f not in ma_features + spread_features + slope_features]\n",
    "\n",
    "print(f\"\\nFeature breakdown:\")\n",
    "print(f\"  MA values: {len(ma_features)}\")\n",
    "print(f\"  Spread features: {len(spread_features)}\")\n",
    "print(f\"  Slope features: {len(slope_features)}\")\n",
    "print(f\"  Other: {len(other_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare Training Data\n",
    "\n",
    "We use time-series split to avoid look-ahead bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with NaN in features or target\n",
    "df_clean = df.dropna(subset=feature_cols + ['signal']).copy()\n",
    "print(f\"Clean samples: {len(df_clean):,} (removed {len(df) - len(df_clean):,} with NaN)\")\n",
    "\n",
    "# Prepare X and y\n",
    "X = df_clean[feature_cols].values\n",
    "y = df_clean['signal'].values\n",
    "\n",
    "# Time-series split (80% train, 20% test - no shuffle!)\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "print(f\"\\nTrain set: {len(X_train):,} samples\")\n",
    "print(f\"Test set: {len(X_test):,} samples\")\n",
    "\n",
    "print(f\"\\nTrain distribution:\")\n",
    "for s, name in signal_map.items():\n",
    "    count = (y_train == s).sum()\n",
    "    print(f\"  {name}: {count:,} ({count/len(y_train)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTest distribution:\")\n",
    "for s, name in signal_map.items():\n",
    "    count = (y_test == s).sum()\n",
    "    print(f\"  {name}: {count:,} ({count/len(y_test)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Features scaled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train Signal Detection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost\n",
    "print(\"Training XGBoost classifier...\")\n",
    "\n",
    "model = xgb.XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    eval_metric='mlogloss',\n",
    "    use_label_encoder=False,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "model.fit(X_train_scaled, y_train)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_proba = model.predict_proba(X_test_scaled)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test, y_pred, target_names=['No Trade', 'Long', 'Short']))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=cm,\n",
    "    x=['No Trade', 'Long', 'Short'],\n",
    "    y=['No Trade', 'Long', 'Short'],\n",
    "    text=cm,\n",
    "    texttemplate='%{text}',\n",
    "    colorscale='Blues'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Confusion Matrix - Signal Detection',\n",
    "    xaxis_title='Predicted',\n",
    "    yaxis_title='Actual',\n",
    "    width=500,\n",
    "    height=500\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 20 Most Important Features:\")\n",
    "print(importance_df.head(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top features\n",
    "top_n = 25\n",
    "top_features = importance_df.head(top_n)\n",
    "\n",
    "fig = go.Figure(go.Bar(\n",
    "    x=top_features['importance'],\n",
    "    y=top_features['feature'],\n",
    "    orientation='h'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f'Top {top_n} Feature Importances',\n",
    "    xaxis_title='Importance',\n",
    "    yaxis_title='Feature',\n",
    "    height=700,\n",
    "    yaxis={'categoryorder': 'total ascending'}\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Signal Quality Analysis\n",
    "\n",
    "Analyze how well the model's signals would have performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add predictions to test data\n",
    "df_test = df_clean.iloc[split_idx:].copy()\n",
    "df_test['pred_signal'] = y_pred\n",
    "df_test['pred_prob_long'] = y_proba[:, 1]\n",
    "df_test['pred_prob_short'] = y_proba[:, 2]\n",
    "\n",
    "# Analyze predicted signals\n",
    "print(\"Predicted Signal Analysis:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for signal_type, signal_name in [(1, 'Long'), (2, 'Short')]:\n",
    "    pred_signals = df_test[df_test['pred_signal'] == signal_type]\n",
    "    if len(pred_signals) > 0:\n",
    "        actual_good = (pred_signals['signal'] == signal_type).mean() * 100\n",
    "        if signal_type == 1:\n",
    "            avg_return = pred_signals['future_return_up'].mean()\n",
    "        else:\n",
    "            avg_return = pred_signals['future_return_down'].mean()\n",
    "        \n",
    "        print(f\"\\n{signal_name} Signals:\")\n",
    "        print(f\"  Predicted: {len(pred_signals):,}\")\n",
    "        print(f\"  Accuracy (actual good entry): {actual_good:.1f}%\")\n",
    "        print(f\"  Avg potential return: {avg_return:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High-confidence signals only\n",
    "CONFIDENCE_THRESHOLD = 0.6\n",
    "\n",
    "print(f\"\\nHigh-Confidence Signals (prob > {CONFIDENCE_THRESHOLD}):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Long signals\n",
    "high_conf_long = df_test[df_test['pred_prob_long'] > CONFIDENCE_THRESHOLD]\n",
    "if len(high_conf_long) > 0:\n",
    "    accuracy = (high_conf_long['signal'] == 1).mean() * 100\n",
    "    avg_return = high_conf_long['future_return_up'].mean()\n",
    "    print(f\"\\nLong Signals:\")\n",
    "    print(f\"  Count: {len(high_conf_long):,}\")\n",
    "    print(f\"  Accuracy: {accuracy:.1f}%\")\n",
    "    print(f\"  Avg potential return: {avg_return:.2f}%\")\n",
    "\n",
    "# Short signals\n",
    "high_conf_short = df_test[df_test['pred_prob_short'] > CONFIDENCE_THRESHOLD]\n",
    "if len(high_conf_short) > 0:\n",
    "    accuracy = (high_conf_short['signal'] == 2).mean() * 100\n",
    "    avg_return = high_conf_short['future_return_down'].mean()\n",
    "    print(f\"\\nShort Signals:\")\n",
    "    print(f\"  Count: {len(high_conf_short):,}\")\n",
    "    print(f\"  Accuracy: {accuracy:.1f}%\")\n",
    "    print(f\"  Avg potential return: {avg_return:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Signals on Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot last portion of test data with signals\n",
    "n_display = 500\n",
    "df_plot = df_test.tail(n_display).copy()\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=1,\n",
    "    shared_xaxes=True,\n",
    "    vertical_spacing=0.05,\n",
    "    row_heights=[0.7, 0.3],\n",
    "    subplot_titles=('Price & Signals', 'Signal Confidence')\n",
    ")\n",
    "\n",
    "# Candlestick\n",
    "fig.add_trace(go.Candlestick(\n",
    "    x=df_plot['open_time'],\n",
    "    open=df_plot['open'], high=df_plot['high'],\n",
    "    low=df_plot['low'], close=df_plot['close'],\n",
    "    name='Price'\n",
    "), row=1, col=1)\n",
    "\n",
    "# Long signals\n",
    "long_signals = df_plot[df_plot['pred_prob_long'] > CONFIDENCE_THRESHOLD]\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=long_signals['open_time'],\n",
    "    y=long_signals['low'] * 0.998,\n",
    "    mode='markers',\n",
    "    marker=dict(symbol='triangle-up', size=12, color='green'),\n",
    "    name=f'Long Signal (p>{CONFIDENCE_THRESHOLD})'\n",
    "), row=1, col=1)\n",
    "\n",
    "# Short signals\n",
    "short_signals = df_plot[df_plot['pred_prob_short'] > CONFIDENCE_THRESHOLD]\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=short_signals['open_time'],\n",
    "    y=short_signals['high'] * 1.002,\n",
    "    mode='markers',\n",
    "    marker=dict(symbol='triangle-down', size=12, color='red'),\n",
    "    name=f'Short Signal (p>{CONFIDENCE_THRESHOLD})'\n",
    "), row=1, col=1)\n",
    "\n",
    "# Confidence plot\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_plot['open_time'],\n",
    "    y=df_plot['pred_prob_long'],\n",
    "    mode='lines',\n",
    "    name='Long Prob',\n",
    "    line=dict(color='green', width=1)\n",
    "), row=2, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_plot['open_time'],\n",
    "    y=df_plot['pred_prob_short'],\n",
    "    mode='lines',\n",
    "    name='Short Prob',\n",
    "    line=dict(color='red', width=1)\n",
    "), row=2, col=1)\n",
    "\n",
    "fig.add_hline(y=CONFIDENCE_THRESHOLD, line_dash='dash', line_color='white', row=2, col=1)\n",
    "\n",
    "fig.update_layout(\n",
    "    template='plotly_dark',\n",
    "    height=800,\n",
    "    xaxis_rangeslider_visible=False,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.update_yaxes(title_text='Price', row=1, col=1)\n",
    "fig.update_yaxes(title_text='Probability', range=[0, 1], row=2, col=1)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and scaler\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_path = MODELS_DIR / f'{SYMBOL}_{INTERVAL}_signal_model.joblib'\n",
    "scaler_path = MODELS_DIR / f'{SYMBOL}_{INTERVAL}_signal_scaler.joblib'\n",
    "\n",
    "joblib.dump(model, model_path)\n",
    "joblib.dump(scaler, scaler_path)\n",
    "\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "print(f\"Scaler saved to: {scaler_path}\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'symbol': SYMBOL,\n",
    "    'interval': INTERVAL,\n",
    "    'created_at': datetime.now().isoformat(),\n",
    "    'feature_cols': feature_cols,\n",
    "    'target_params': {\n",
    "        'lookahead_bars': LOOKAHEAD_BARS,\n",
    "        'profit_target_pct': PROFIT_TARGET_PCT,\n",
    "        'stop_loss_pct': STOP_LOSS_PCT\n",
    "    },\n",
    "    'train_samples': len(X_train),\n",
    "    'test_samples': len(X_test)\n",
    "}\n",
    "\n",
    "metadata_path = MODELS_DIR / f'{SYMBOL}_{INTERVAL}_signal_metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"Metadata saved to: {metadata_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Quick Inference Function\n",
    "\n",
    "Function to generate signals on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_signals(df, model, scaler, feature_cols, confidence_threshold=0.6):\n",
    "    \"\"\"\n",
    "    Generate trading signals for new data.\n",
    "    \n",
    "    Returns DataFrame with signal columns added.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Prepare features\n",
    "    X = df[feature_cols].values\n",
    "    X_scaled = scaler.transform(X)\n",
    "    \n",
    "    # Predict\n",
    "    proba = model.predict_proba(X_scaled)\n",
    "    \n",
    "    df['prob_no_trade'] = proba[:, 0]\n",
    "    df['prob_long'] = proba[:, 1]\n",
    "    df['prob_short'] = proba[:, 2]\n",
    "    \n",
    "    # High-confidence signals\n",
    "    df['signal'] = 0\n",
    "    df.loc[df['prob_long'] > confidence_threshold, 'signal'] = 1\n",
    "    df.loc[df['prob_short'] > confidence_threshold, 'signal'] = 2\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"Inference function defined.\")\n",
    "print(\"\\nUsage:\")\n",
    "print(\"  df_with_signals = predict_signals(df, model, scaler, feature_cols)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook trained a signal detection model that:\n",
    "1. Uses MAR indicator features to predict good entry points\n",
    "2. Labels are based on actual future price movement (not manual)\n",
    "3. Outputs probability scores for long/short/no-trade\n",
    "4. Can filter by confidence threshold for higher quality signals\n",
    "\n",
    "**Next Steps:**\n",
    "- Use **04_backtest.ipynb** to test the complete strategy\n",
    "- Tune target parameters (lookahead, profit target, stop loss)\n",
    "- Experiment with different confidence thresholds\n",
    "- Add regime filtering (only long in uptrend, only short in downtrend)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
